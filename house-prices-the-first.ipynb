{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House prices: The First \n\nFork from:  \n* [House prices: Easy mode (top 12%)](https://www.kaggle.com/code/matthieugouel/house-prices-easy-mode-top-12/notebook)\n\nSources that helped me a lot:\n* [Stacked Ensemble Models (Top 3% on Leaderboard)](https://www.kaggle.com/code/alexturkmen/preprocessing-modeling-with-stacking-top-5#2---Preprocessing)\n* [Preprocessing & Modeling with Stacking -->Top 5%](https://www.kaggle.com/code/limyenwee/stacked-ensemble-models-top-3-on-leaderboard)","metadata":{}},{"cell_type":"markdown","source":"## Initialization\n\nIn this section we import the dataset and the required packages.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-09-27T03:39:02.278510Z","iopub.execute_input":"2022-09-27T03:39:02.279110Z","iopub.status.idle":"2022-09-27T03:39:02.314278Z","shell.execute_reply.started":"2022-09-27T03:39:02.278984Z","shell.execute_reply":"2022-09-27T03:39:02.312664Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler, OrdinalEncoder\nfrom sklearn.metrics import mean_squared_log_error, r2_score\n\nfrom xgboost import XGBRegressor\n\nfrom shap import Explainer\n\n# Dataset directory\nbase_path = Path(\"../input/house-prices-advanced-regression-techniques\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T03:39:02.316964Z","iopub.execute_input":"2022-09-27T03:39:02.318234Z","iopub.status.idle":"2022-09-27T03:39:07.058068Z","shell.execute_reply.started":"2022-09-27T03:39:02.318189Z","shell.execute_reply":"2022-09-27T03:39:07.056647Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(base_path / \"train.csv\")\ndf_test = pd.read_csv(base_path / \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T03:39:07.059928Z","iopub.execute_input":"2022-09-27T03:39:07.060867Z","iopub.status.idle":"2022-09-27T03:39:07.155457Z","shell.execute_reply.started":"2022-09-27T03:39:07.060790Z","shell.execute_reply":"2022-09-27T03:39:07.154086Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(columns=['SalePrice', 'Id'])\ny = train_df['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=8888)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T02:20:46.495440Z","iopub.execute_input":"2022-09-27T02:20:46.495860Z","iopub.status.idle":"2022-09-27T02:20:46.522394Z","shell.execute_reply.started":"2022-09-27T02:20:46.495828Z","shell.execute_reply":"2022-09-27T02:20:46.521567Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/857286249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8888\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"],"ename":"NameError","evalue":"name 'train_df' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\nIn this section we explore the data to help the feature engineering process.  \nWe can remark several things:\n* There is numerical and categorical features\n* There are missing values\n* The numerical values are not scaled\n* The target values (SalePrice) is skewed\n\n\nOf course we could go further on the investigation (check the colinearity, information gain, ...) \nbut we choose here to stay light and general. \n","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:47.018453Z","iopub.execute_input":"2022-08-29T11:14:47.01926Z","iopub.status.idle":"2022-08-29T11:14:47.050689Z","shell.execute_reply.started":"2022-08-29T11:14:47.019224Z","shell.execute_reply":"2022-08-29T11:14:47.049168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"SalePrice\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:47.052827Z","iopub.execute_input":"2022-08-29T11:14:47.053269Z","iopub.status.idle":"2022-08-29T11:14:47.065812Z","shell.execute_reply.started":"2022-08-29T11:14:47.05323Z","shell.execute_reply":"2022-08-29T11:14:47.06467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df_train[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:47.067629Z","iopub.execute_input":"2022-08-29T11:14:47.068248Z","iopub.status.idle":"2022-08-29T11:14:47.513197Z","shell.execute_reply.started":"2022-08-29T11:14:47.068212Z","shell.execute_reply":"2022-08-29T11:14:47.511649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train[\"SalePrice\"].skew())\nprint(df_train[\"SalePrice\"].kurt())","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:47.515054Z","iopub.execute_input":"2022-08-29T11:14:47.516045Z","iopub.status.idle":"2022-08-29T11:14:47.524989Z","shell.execute_reply.started":"2022-08-29T11:14:47.515993Z","shell.execute_reply":"2022-08-29T11:14:47.523326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize missing values\ntotal = X.isnull().sum().sort_values(ascending=False)\npercent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n\nmissing_data=missing_data.head(35)\nf, ax = plt.subplots(figsize=(16, 8))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.title('Percent missing data by feature', fontsize=15)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:48.486273Z","iopub.execute_input":"2022-08-29T11:14:48.48719Z","iopub.status.idle":"2022-08-29T11:14:49.118994Z","shell.execute_reply.started":"2022-08-29T11:14:48.487142Z","shell.execute_reply":"2022-08-29T11:14:49.117643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\nIn this section, we perform some basic feature engineering: \n\n* Split features into numerical and categorical data\n* Fill missing numerical values with mean\n* Fill missing categorical vues with a \"Missing\" category\n* Scale numerical features \n* Encode categorical features into numbers\n* Apply a log transformation to target values to mitigate the skewness","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.121152Z","iopub.execute_input":"2022-08-29T11:14:49.121494Z","iopub.status.idle":"2022-08-29T11:14:49.13241Z","shell.execute_reply.started":"2022-08-29T11:14:49.121458Z","shell.execute_reply":"2022-08-29T11:14:49.130986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get numerical and categorical features\nnumeric_feats = X.dtypes[X.dtypes != \"object\"].index\ncategoric_feats = X.dtypes[X.dtypes == \"object\"].index\n\n# Get features with missing values\nna_numeric_feats = [k for k, v in X[numeric_feats].isnull().sum().to_dict().items() if v > 0]\nna_categoric_feats = [k for k, v in X[categoric_feats].isnull().sum().to_dict().items() if v > 0]\n\nprint(na_numeric_feats)\nprint(na_categoric_feats)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.134554Z","iopub.execute_input":"2022-08-29T11:14:49.134901Z","iopub.status.idle":"2022-08-29T11:14:49.153046Z","shell.execute_reply.started":"2022-08-29T11:14:49.134861Z","shell.execute_reply":"2022-08-29T11:14:49.152224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean numerical features with missing values\nimp = SimpleImputer(strategy=\"mean\")\nX_train[na_numeric_feats] = imp.fit_transform(X_train[na_numeric_feats])\nX_val[na_numeric_feats] = imp.transform(X_val[na_numeric_feats])\n\n# Clean categorical features with missing values\nfor feat in na_categoric_feats:\n    X_train[feat].fillna(\"Missing\", inplace=True)\n    X_val[feat].fillna(\"Missing\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.154455Z","iopub.execute_input":"2022-08-29T11:14:49.155073Z","iopub.status.idle":"2022-08-29T11:14:49.181095Z","shell.execute_reply.started":"2022-08-29T11:14:49.154971Z","shell.execute_reply":"2022-08-29T11:14:49.180022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale numerical features\nfor feat in numeric_feats:\n    scaler = RobustScaler()\n    X_train[feat] = scaler.fit_transform(X_train[feat].values.reshape(-1, 1))\n    X_val[feat] = scaler.transform(X_val[feat].values.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.182581Z","iopub.execute_input":"2022-08-29T11:14:49.183192Z","iopub.status.idle":"2022-08-29T11:14:49.241699Z","shell.execute_reply.started":"2022-08-29T11:14:49.183157Z","shell.execute_reply":"2022-08-29T11:14:49.240772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode categorical features\nfor feat in categoric_feats:\n    encoder = OrdinalEncoder()\n    X_train[feat] = encoder.fit_transform(X_train[feat].values.reshape(-1, 1))\n    X_val[feat] = encoder.fit_transform(X_val[feat].values.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.243127Z","iopub.execute_input":"2022-08-29T11:14:49.243698Z","iopub.status.idle":"2022-08-29T11:14:49.322995Z","shell.execute_reply.started":"2022-08-29T11:14:49.243665Z","shell.execute_reply":"2022-08-29T11:14:49.322009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Log-transformation of skewed target variable\ny_train = np.log1p(y_train)\ny_val = np.log1p(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.324469Z","iopub.execute_input":"2022-08-29T11:14:49.325064Z","iopub.status.idle":"2022-08-29T11:14:49.330027Z","shell.execute_reply.started":"2022-08-29T11:14:49.32502Z","shell.execute_reply":"2022-08-29T11:14:49.329198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n\nIn this section we model our regressor and evaluate it.  \nWe only use XGBoost ensemble regressor, and not perform stacking/blending of different models, for simplicity.\n\nThe hyper parameter tuning has been applied and then commented.","metadata":{}},{"cell_type":"code","source":"# # Hyperparameter Tuning for XGBoost\n# from sklearn.model_selection import GridSearchCV\n\n# parameters = {\n#     \"n_estimators\": [500, 750, 1000, 1500, 2000], \n#     \"learning_rate\": [0.01, 0.02, 0.05], \n#     \"max_depth\": [6, 8], \n#     \"subsample\": [0.3, 0.5, 0.7]\n# }\n\n# grid = GridSearchCV(XGBRegressor(objective='reg:squarederror'), parameters)\n# grid.fit(X_train, y_train)\n\n# print(grid.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.352771Z","iopub.execute_input":"2022-08-29T11:14:49.353565Z","iopub.status.idle":"2022-08-29T11:14:49.361484Z","shell.execute_reply.started":"2022-08-29T11:14:49.35353Z","shell.execute_reply":"2022-08-29T11:14:49.360596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model with training data\nmodel = XGBRegressor(n_estimators=1500, learning_rate=0.02, max_depth=6, subsample=0.7)\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:14:49.362827Z","iopub.execute_input":"2022-08-29T11:14:49.363888Z","iopub.status.idle":"2022-08-29T11:15:01.776726Z","shell.execute_reply.started":"2022-08-29T11:14:49.363852Z","shell.execute_reply":"2022-08-29T11:15:01.775174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-----\")\nprint(\"* Training set\")\ny_pred = model.predict(X_train)\nprint(f\"R2: {r2_score(y_train, y_pred):.2%}\")\nprint(f\"RMSE: {mean_squared_log_error(y_train, y_pred, squared=False):.5f}\")\n\nprint(\"-----\")\nprint(\"* Validation set\")\ny_pred = model.predict(X_val)\nprint(f\"R2: {r2_score(y_val, y_pred):.2%}\")\nprint(f\"RMSE: {mean_squared_log_error(y_val, y_pred, squared=False):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-29T11:15:01.77869Z","iopub.execute_input":"2022-08-29T11:15:01.779275Z","iopub.status.idle":"2022-08-29T11:15:01.855313Z","shell.execute_reply.started":"2022-08-29T11:15:01.779211Z","shell.execute_reply":"2022-08-29T11:15:01.854052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}